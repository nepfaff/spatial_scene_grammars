## Sampling instructions

1. Execute the following script (make sure to adjust `dataset_save_file` in the script):
```bash
python generate_target_dataset_parallel.py
```

**NOTE:** Need to use `combine_extracted_datasets.py` even if have only one file with
`--extract=True` (default option).

2. Extract and filter the dataset:
```bash
python extract_dataset.py tri_table_scenes_50k_batch1.pkl
```
where `tri_table_scenes_50k_batch1.pkl` is the path of the file generated by `generate_target_dataset_parallel.py`.

Alternatively, you can ran this command for all files in a folder using the following:
```bash
find /path/to/your/folder -name "*.pkl" | xargs -P 12 -I {} python extract_dataset.py "{}"
```
where `-P 12` is the number of parallel processes to run.

3. Move the resultin files into a folder.

4. Combine all the datasets in the folder:
```bash
python combine_extracted_datasets.py path/to/folder output_path.pkl
```

## Shared memory issues during sampling

If you get the following error, then you ran out of shared memory.
```
RuntimeError: unable to mmap 104 bytes from file </torch_48335_894824247_52>: Cannot allocate memory (12)
```

Increase the max map count:
```
sudo sysctl -w vm.max_map_count=262144
```